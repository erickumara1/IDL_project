# -*- coding: utf-8 -*-
"""finetuning_using_our_own_bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w7t9K3ALbYPqSbSBveRGhZCkXX4219lN
"""

# Instead of loading from TF Hub
import torch
from torch import nn
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
from transformers import BertTokenizer

# 1. Load your custom BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/email_bert/tokenizer/bert-email-vocab.txt')

# Load the pre-trained model
class BERTClassifier(nn.Module):
    def __init__(self, bert_model):
        super(BERTClassifier, self).__init__()
        self.bert = bert_model  # This is your email-trained BERT
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(768, 1)  # 768 is the BERT hidden size

    def forward(self, input_ids, attention_mask, token_type_ids):
        # Get BERT outputs
        outputs = self.bert(input_ids=input_ids,
                           attention_mask=attention_mask,
                           token_type_ids=token_type_ids)

        # Use the [CLS] token representation for classification
        pooled_output = outputs[1]  # This might need adjustment based on your model's output

        # Apply dropout and classification layer
        x = self.dropout(pooled_output)
        logits = self.classifier(x)
        return logits

# Load your pre-trained BERT model
from transformers import BertModel
bert_model = BertModel.from_pretrained('/content/drive/MyDrive/email_bert/bert_email_final.pth')

# Create classifier model
classifier_model = BERTClassifier(bert_model)

# 2. Data preparation functions
def prepare_data(texts, labels=None, max_length=128):
    encodings = tokenizer(texts.tolist(),
                         truncation=True,
                         padding='max_length',
                         max_length=max_length,
                         return_tensors='pt')

    dataset = {
        'input_ids': encodings['input_ids'],
        'attention_mask': encodings['attention_mask'],
        'token_type_ids': encodings['token_type_ids']
    }

    if labels is not None:
        dataset['labels'] = torch.tensor(labels.values, dtype=torch.float)

    return dataset

# 3. Training setup
from torch.utils.data import DataLoader, TensorDataset
import torch.optim as optim

# Prepare datasets
train_dataset = prepare_data(X_train, y_train)
test_dataset = prepare_data(X_test, y_test)

# Create tensor datasets
train_tensor_dataset = TensorDataset(
    train_dataset['input_ids'],
    train_dataset['attention_mask'],
    train_dataset['token_type_ids'],
    train_dataset['labels']
)

test_tensor_dataset = TensorDataset(
    test_dataset['input_ids'],
    test_dataset['attention_mask'],
    test_dataset['token_type_ids'],
    test_dataset['labels']
)

# Create dataloaders
train_loader = DataLoader(train_tensor_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_tensor_dataset, batch_size=16)

# Define loss function and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(classifier_model.parameters(), lr=3e-5)

# 4. Training loop
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
classifier_model.to(device)

num_epochs = 10
for epoch in range(num_epochs):
    classifier_model.train()
    running_loss = 0.0

    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(train_loader):
        # Move tensors to the configured device
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        token_type_ids = token_type_ids.to(device)
        labels = labels.to(device)

        # Forward pass
        outputs = classifier_model(input_ids, attention_mask, token_type_ids)
        loss = criterion(outputs.squeeze(), labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')

# 5. Evaluation
classifier_model.eval()
y_true = []
y_pred = []

with torch.no_grad():
    for input_ids, attention_mask, token_type_ids, labels in test_loader:
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        token_type_ids = token_type_ids.to(device)

        outputs = classifier_model(input_ids, attention_mask, token_type_ids)
        predicted = torch.sigmoid(outputs.squeeze()) > 0.5

        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Print metrics
cm = confusion_matrix(y_true, y_pred)
print(cm)
print(classification_report(y_true, y_pred))
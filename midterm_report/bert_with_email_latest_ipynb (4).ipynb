{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiJB5X7CI2xt",
        "outputId": "4a8465ef-1e0c-456d-ec63-58bd4e821079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/email_bert\n",
        "!mkdir -p /content/drive/MyDrive/email_bert/datasets\n",
        "!mkdir -p /content/drive/MyDrive/email_bert/data\n",
        "!mkdir -p /content/drive/MyDrive/email_bert/tokenizer\n",
        "!mkdir -p /content/drive/MyDrive/email_bert/checkpoints\n",
        "\n",
        "!pip install transformers datasets tokenizers pandas\n",
        "# (you don't actually need `email-parser` – you're using Python's built-in `email` module)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download once to local disk\n",
        "!wget https://www.cs.cmu.edu/~enron/enron_mail_20150507.tar.gz -O /content/enron_mail_20150507.tar.gz\n",
        "\n",
        "# Optional: keep a backup in Drive\n",
        "!cp /content/enron_mail_20150507.tar.gz /content/drive/MyDrive/email_bert/datasets/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JbJxm6ZNvRG",
        "outputId": "a1888ae6-30bf-4fea-d42b-ee14755b7646"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-02 04:33:04--  https://www.cs.cmu.edu/~enron/enron_mail_20150507.tar.gz\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 443254787 (423M) [application/x-gzip]\n",
            "Saving to: ‘/content/enron_mail_20150507.tar.gz’\n",
            "\n",
            "/content/enron_mail 100%[===================>] 422.72M   577KB/s    in 13m 21s \n",
            "\n",
            "2025-12-02 04:46:26 (540 KB/s) - ‘/content/enron_mail_20150507.tar.gz’ saved [443254787/443254787]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/email_bert\n",
        "!mkdir -p /content/drive/MyDrive/email_bert/datasets\n",
        "!mkdir -p /content/drive/MyDrive/email_bert/data\n",
        "!mkdir -p /content/drive/MyDrive/email_bert/tokenizer\n",
        "!mkdir -p /content/drive/MyDrive/email_bert/checkpoints\n"
      ],
      "metadata": {
        "id": "6kRBRLHtZYVj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf /content/enron_mail_20150507.tar.gz -C /content/\n",
        "\n",
        "enron_dir = '/content/maildir/'\n",
        "\n",
        "# Then we'll work with the locally extracted files\n",
        "\n",
        "\n",
        "# Now we can continue with the email processing\n",
        "# Change your code to use this path"
      ],
      "metadata": {
        "id": "pgqWohFGSYNG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Extract the downloaded tarball\n",
        "# enron_dir = '/content/maildir/'\n",
        "\n",
        "\n",
        "# # Data processing: Process email data instead of movie dialogues\n",
        "# import os\n",
        "# import email\n",
        "# import pandas as pd\n",
        "# from pathlib import Path\n",
        "# import torch\n",
        "# import re\n",
        "# import random\n",
        "# import transformers, datasets\n",
        "# from tokenizers import BertWordPieceTokenizer\n",
        "# from transformers import BertTokenizer\n",
        "# import tqdm\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import itertools\n",
        "# import math\n",
        "# import torch.nn.functional as F\n",
        "# import numpy as np\n",
        "# from torch.optim import Adam\n",
        "# from email.parser import Parser\n",
        "# import json\n",
        "\n",
        "# def extract_email_content(email_path):\n",
        "#     \"\"\"Extract content from an email file\"\"\"\n",
        "#     try:\n",
        "#         with open(email_path, 'r', encoding='latin1') as f:\n",
        "#             content = f.read()\n",
        "\n",
        "#         # Parse email content\n",
        "#         msg = email.message_from_string(content)\n",
        "\n",
        "#         # Get subject and body\n",
        "#         subject = msg.get('Subject', '')\n",
        "\n",
        "#         body = \"\"\n",
        "#         if msg.is_multipart():\n",
        "#             for part in msg.walk():\n",
        "#                 ctype = part.get_content_type()\n",
        "#                 cdispo = str(part.get('Content-Disposition'))\n",
        "\n",
        "#                 # Skip attachments\n",
        "#                 if ctype == 'text/plain' and 'attachment' not in cdispo:\n",
        "#                     part_body = part.get_payload(decode=True)\n",
        "#                     if part_body:\n",
        "#                         body = part_body.decode('latin1', errors='ignore')\n",
        "#                     break\n",
        "#         else:\n",
        "#             part_body = msg.get_payload(decode=True)\n",
        "#             if part_body:\n",
        "#                 body = part_body.decode('latin1', errors='ignore')\n",
        "\n",
        "#         # Clean text - remove excessive whitespace\n",
        "#         body = re.sub(r'\\s+', ' ', body).strip() if body else \"\"\n",
        "#         subject = re.sub(r'\\s+', ' ', subject).strip()\n",
        "\n",
        "#         return subject, body\n",
        "#     except Exception as e:\n",
        "#         return \"\", \"\"\n",
        "\n",
        "# # Process Enron emails\n",
        "# print(\"Processing Enron emails...\")\n",
        "# MAX_LEN = 64\n",
        "# #enron_dir = '/content/drive/MyDrive/email_bert/datasets/enron_mail_20150507/maildir/'\n",
        "# email_pairs = []\n",
        "\n",
        "# # Limit to a sample to avoid processing too many emails\n",
        "# users = os.listdir(enron_dir)[:30]  # Take first 30 users\n",
        "\n",
        "# for user in users:\n",
        "#     user_dir = os.path.join(enron_dir, user)\n",
        "#     if os.path.isdir(user_dir):\n",
        "#         for folder in os.listdir(user_dir)[:5]:  # Limit folders per user\n",
        "#             folder_path = os.path.join(user_dir, folder)\n",
        "#             if os.path.isdir(folder_path):\n",
        "#                 for file in os.listdir(folder_path)[:50]:  # Limit files per folder\n",
        "#                     file_path = os.path.join(folder_path, file)\n",
        "#                     if os.path.isfile(file_path):\n",
        "#                         subject, body = extract_email_content(file_path)\n",
        "\n",
        "#                         # Skip empty emails\n",
        "#                         if not body:\n",
        "#                             continue\n",
        "\n",
        "#                         # Create sentence pairs for NSP\n",
        "#                         # 1. Split email body into paragraphs\n",
        "#                         paragraphs = body.split('\\n')\n",
        "#                         paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
        "\n",
        "#                         if len(paragraphs) < 2:\n",
        "#                             # If no clear paragraphs, split by sentences\n",
        "#                             sentences = re.split(r'[.!?]+', body)\n",
        "#                             sentences = [s.strip() for s in sentences if len(s.strip()) > 20]\n",
        "\n",
        "#                             if len(sentences) >= 2:\n",
        "#                                 # Create pairs from consecutive sentences\n",
        "#                                 for i in range(len(sentences) - 1):\n",
        "#                                     # Positive example: consecutive sentences\n",
        "#                                     first = sentences[i]\n",
        "#                                     second = sentences[i + 1]\n",
        "\n",
        "#                                     if len(first.split()) > 3 and len(second.split()) > 3:\n",
        "#                                         email_pairs.append([\n",
        "#                                             ' '.join(first.split()[:MAX_LEN]),\n",
        "#                                             ' '.join(second.split()[:MAX_LEN])\n",
        "#                                         ])\n",
        "#                         else:\n",
        "#                             # Create pairs from paragraphs\n",
        "#                             for i in range(len(paragraphs) - 1):\n",
        "#                                 # Positive example: consecutive paragraphs\n",
        "#                                 first = paragraphs[i]\n",
        "#                                 second = paragraphs[i + 1]\n",
        "\n",
        "#                                 if len(first.split()) > 3 and len(second.split()) > 3:\n",
        "#                                     email_pairs.append([\n",
        "#                                         ' '.join(first.split()[:MAX_LEN]),\n",
        "#                                         ' '.join(second.split()[:MAX_LEN])\n",
        "#                                     ])\n",
        "\n",
        "# print(f\"Created {len(email_pairs)} email pairs\")\n",
        "# # Sample one pair to verify\n",
        "# print(\"Sample pair:\", email_pairs[20] if len(email_pairs) > 20 else email_pairs[0])\n",
        "\n",
        "# # Save pairs to Drive for reuse\n",
        "# import pickle\n",
        "# with open('/content/drive/MyDrive/email_bert/datasets/email_pairs.pkl', 'wb') as f:\n",
        "#     pickle.dump(email_pairs, f)\n",
        "\n",
        "# # Save data as txt file for tokenizer training\n",
        "# text_data = []\n",
        "# file_count = 0\n",
        "\n",
        "# for sample in tqdm.tqdm([x[0] for x in email_pairs] + [x[1] for x in email_pairs]):\n",
        "#     text_data.append(sample)\n",
        "\n",
        "#     # Save to file once we hit the 10K mark\n",
        "#     if len(text_data) == 10000:\n",
        "#         with open(f'/content/drive/MyDrive/email_bert/data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
        "#             fp.write('\\n'.join(text_data))\n",
        "#         text_data = []\n",
        "#         file_count += 1\n",
        "\n",
        "# # Save remaining data\n",
        "# if text_data:\n",
        "#     with open(f'/content/drive/MyDrive/email_bert/data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
        "#         fp.write('\\n'.join(text_data))\n",
        "\n",
        "# paths = [str(x) for x in Path('/content/drive/MyDrive/email_bert/data').glob('**/*.txt')]\n",
        "# print(f\"Created {len(paths)} text files for tokenizer training\")\n",
        "\n",
        "# # Train tokenizer\n",
        "# tokenizer = BertWordPieceTokenizer(\n",
        "#     clean_text=True,\n",
        "#     handle_chinese_chars=False,\n",
        "#     strip_accents=False,\n",
        "#     lowercase=True\n",
        "# )\n",
        "\n",
        "# tokenizer.train(\n",
        "#     files=paths,\n",
        "#     vocab_size=30_000,\n",
        "#     min_frequency=2,\n",
        "#     limit_alphabet=1000,\n",
        "#     wordpieces_prefix='##',\n",
        "#     special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
        "# )\n",
        "\n",
        "# # Save tokenizer to Drive\n",
        "# # Save tokenizer\n",
        "# tokenizer.save_model('/content/drive/MyDrive/email_bert/tokenizer', 'bert-email')\n",
        "\n",
        "# # Load tokenizer (CORRECT way)\n",
        "# from transformers import BertTokenizerFast\n",
        "\n",
        "# tokenizer = BertTokenizerFast(\n",
        "#     vocab_file=\"/content/drive/MyDrive/email_bert/tokenizer/bert-email-vocab.txt\",\n",
        "#     lowercase=True,\n",
        "#     strip_accents=False\n",
        "# )\n",
        "\n",
        "# # Test tokenizer\n",
        "# sample_text = \"This is an email about the project deadline tomorrow.\"\n",
        "# token_ids = tokenizer(sample_text)['input_ids']\n",
        "# print(\"Sample tokenization:\", tokenizer.convert_ids_to_tokens(token_ids))\n",
        "\n",
        "\n",
        "# import os\n",
        "# import math\n",
        "# import random\n",
        "# import itertools\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.optim import Adam\n",
        "# import tqdm\n",
        "# import wandb\n",
        "\n",
        "# # =====================================\n",
        "# # 1) BERTDataset (same interface as before)\n",
        "# # =====================================\n",
        "\n",
        "# class BERTDataset(Dataset):\n",
        "#     def __init__(self, data_pair, tokenizer, seq_len=64):\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.seq_len = seq_len\n",
        "#         self.corpus_lines = len(data_pair)\n",
        "#         self.lines = data_pair\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.corpus_lines\n",
        "\n",
        "#     def __getitem__(self, item):\n",
        "#         # Step 1: get email pair, either negative or positive (for NSP)\n",
        "#         t1, t2, is_next_label = self.get_sent(item)\n",
        "\n",
        "#         # Step 2: replace random words with mask / random words (for MLM)\n",
        "#         t1_random, t1_label = self.random_word(t1)\n",
        "#         t2_random, t2_label = self.random_word(t2)\n",
        "\n",
        "#         # Step 3: Add special tokens (CLS, SEP)\n",
        "#         cls_id = self.tokenizer.vocab['[CLS]']\n",
        "#         sep_id = self.tokenizer.vocab['[SEP]']\n",
        "#         pad_id = self.tokenizer.vocab['[PAD]']\n",
        "\n",
        "#         t1 = [cls_id] + t1_random + [sep_id]\n",
        "#         t2 = t2_random + [sep_id]\n",
        "\n",
        "#         t1_label = [pad_id] + t1_label + [pad_id]\n",
        "#         t2_label = t2_label + [pad_id]\n",
        "\n",
        "#         # Step 4: combine parts and add padding\n",
        "#         segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
        "#         bert_input = (t1 + t2)[:self.seq_len]\n",
        "#         bert_label = (t1_label + t2_label)[:self.seq_len]\n",
        "\n",
        "#         padding_len = self.seq_len - len(bert_input)\n",
        "#         if padding_len > 0:\n",
        "#             padding = [pad_id] * padding_len\n",
        "#             bert_input.extend(padding)\n",
        "#             bert_label.extend(padding)\n",
        "#             segment_label.extend(padding)\n",
        "\n",
        "#         output = {\n",
        "#             \"bert_input\": bert_input,\n",
        "#             \"bert_label\": bert_label,\n",
        "#             \"segment_label\": segment_label,\n",
        "#             \"is_next\": is_next_label\n",
        "#         }\n",
        "\n",
        "#         return {key: torch.tensor(value) for key, value in output.items()}\n",
        "\n",
        "#     def random_word(self, sentence):\n",
        "#         \"\"\"\n",
        "#         Very similar to your original logic:\n",
        "#         - 15% of tokens are selected for MLM\n",
        "#         - 80% -> [MASK]\n",
        "#         - 10% -> random token\n",
        "#         - 10% -> leave as is\n",
        "#         Labels:\n",
        "#         - token id where we predict\n",
        "#         - 0 where we don't predict (ignore_index=0 in MLM loss)\n",
        "#         \"\"\"\n",
        "#         tokens = sentence.split()\n",
        "#         output_label = []\n",
        "#         output = []\n",
        "\n",
        "#         for token in tokens:\n",
        "#             prob = random.random()\n",
        "\n",
        "#             token_ids = self.tokenizer(token)['input_ids']\n",
        "#             # token_ids: [CLS, ..., SEP] or just [CLS, SEP] if weird\n",
        "#             if len(token_ids) <= 2:\n",
        "#                 continue\n",
        "\n",
        "#             token_ids = token_ids[1:-1]  # remove special tokens\n",
        "\n",
        "#             if prob < 0.15:\n",
        "#                 prob /= 0.15\n",
        "\n",
        "#                 # 80%: replace with [MASK]\n",
        "#                 if prob < 0.8:\n",
        "#                     for _ in range(len(token_ids)):\n",
        "#                         output.append(self.tokenizer.vocab['[MASK]'])\n",
        "#                 # 10%: random token\n",
        "#                 elif prob < 0.9:\n",
        "#                     for _ in range(len(token_ids)):\n",
        "#                         output.append(random.randrange(len(self.tokenizer.vocab)))\n",
        "#                 # 10%: keep original\n",
        "#                 else:\n",
        "#                     output.extend(token_ids)\n",
        "\n",
        "#                 output_label.extend(token_ids)\n",
        "#             else:\n",
        "#                 # no prediction for this token\n",
        "#                 output.extend(token_ids)\n",
        "#                 output_label.extend([0] * len(token_ids))\n",
        "\n",
        "#         # Already flat lists, but keep this in case\n",
        "#         output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
        "#         output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
        "\n",
        "#         return output, output_label\n",
        "\n",
        "#     def get_sent(self, index):\n",
        "#         \"\"\"Return email segment pair - either positive or negative example for NSP.\"\"\"\n",
        "#         t1, t2 = self.get_corpus_line(index)\n",
        "\n",
        "#         # 50% chance for positive / negative\n",
        "#         if random.random() > 0.5:\n",
        "#             return t1, t2, 1  # Positive example\n",
        "#         else:\n",
        "#             return t1, self.get_random_line(), 0  # Negative example\n",
        "\n",
        "#     def get_corpus_line(self, item):\n",
        "#         \"\"\"Return segment pair from corpus.\"\"\"\n",
        "#         return self.lines[item][0], self.lines[item][1]\n",
        "\n",
        "#     def get_random_line(self):\n",
        "#         \"\"\"Return random segment.\"\"\"\n",
        "#         i = random.randrange(len(self.lines))\n",
        "#         return self.lines[i][random.choice([0, 1])]\n",
        "\n",
        "\n",
        "# # ==========================\n",
        "# # 2) Embeddings & Encoder\n",
        "# # ==========================\n",
        "\n",
        "# class PositionalEmbedding(torch.nn.Module):\n",
        "#     def __init__(self, d_model, max_len=128):\n",
        "#         super().__init__()\n",
        "#         pe = torch.zeros(max_len, d_model).float()\n",
        "#         pe.requires_grad = False\n",
        "\n",
        "#         for pos in range(max_len):\n",
        "#             for i in range(0, d_model, 2):\n",
        "#                 pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "#                 if i + 1 < d_model:\n",
        "#                     pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "\n",
        "#         # shape: (1, max_len, d_model)\n",
        "#         self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x: (batch_size, seq_len)\n",
        "#         seq_len = x.size(1)\n",
        "#         # Broadcast along batch dimension automatically\n",
        "#         return self.pe[:, :seq_len, :]\n",
        "\n",
        "\n",
        "# class BERTEmbedding(torch.nn.Module):\n",
        "#     \"\"\"\n",
        "#     BERT Embedding:\n",
        "#       - TokenEmbedding\n",
        "#       - SegmentEmbedding\n",
        "#       - PositionalEmbedding\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, vocab_size, embed_size, seq_len=64, dropout=0.1):\n",
        "#         super().__init__()\n",
        "#         self.embed_size = embed_size\n",
        "#         self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "#         self.segment = torch.nn.Embedding(3, embed_size, padding_idx=0)\n",
        "#         self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
        "#         self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "#     def forward(self, sequence, segment_label):\n",
        "#         # sequence, segment_label: (batch_size, seq_len)\n",
        "#         positions = self.position(sequence)         # (1, seq_len, d_model) -> broadcast\n",
        "#         x = self.token(sequence) + positions + self.segment(segment_label)\n",
        "#         return self.dropout(x)\n",
        "\n",
        "\n",
        "# class MultiHeadedAttention(torch.nn.Module):\n",
        "#     def __init__(self, heads, d_model, dropout=0.1):\n",
        "#         super().__init__()\n",
        "#         assert d_model % heads == 0\n",
        "#         self.d_k = d_model // heads\n",
        "#         self.heads = heads\n",
        "#         self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "#         self.query = torch.nn.Linear(d_model, d_model)\n",
        "#         self.key = torch.nn.Linear(d_model, d_model)\n",
        "#         self.value = torch.nn.Linear(d_model, d_model)\n",
        "#         self.output_linear = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "#     def forward(self, query, key, value, mask):\n",
        "#         # query/key/value: (batch_size, seq_len, d_model)\n",
        "#         query = self.query(query)\n",
        "#         key = self.key(key)\n",
        "#         value = self.value(value)\n",
        "\n",
        "#         # (batch_size, seq_len, d_model) -> (batch_size, heads, seq_len, d_k)\n",
        "#         def split_heads(x):\n",
        "#             return x.view(x.size(0), -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "\n",
        "#         query = split_heads(query)\n",
        "#         key = split_heads(key)\n",
        "#         value = split_heads(value)\n",
        "\n",
        "#         # scores: (batch_size, heads, seq_len, seq_len)\n",
        "#         scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(self.d_k)\n",
        "#         scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "#         weights = F.softmax(scores, dim=-1)\n",
        "#         weights = self.dropout(weights)\n",
        "\n",
        "#         # context: (batch_size, heads, seq_len, d_k)\n",
        "#         context = torch.matmul(weights, value)\n",
        "\n",
        "#         # -> (batch_size, seq_len, d_model)\n",
        "#         context = context.permute(0, 2, 1, 3).contiguous()\n",
        "#         context = context.view(context.size(0), -1, self.heads * self.d_k)\n",
        "\n",
        "#         return self.output_linear(context)\n",
        "\n",
        "\n",
        "# class FeedForward(torch.nn.Module):\n",
        "#     \"Implements FFN: Linear -> GELU -> Dropout -> Linear\"\n",
        "\n",
        "#     def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
        "#         super().__init__()\n",
        "#         self.fc1 = torch.nn.Linear(d_model, middle_dim)\n",
        "#         self.fc2 = torch.nn.Linear(middle_dim, d_model)\n",
        "#         self.dropout = torch.nn.Dropout(dropout)\n",
        "#         self.activation = torch.nn.GELU()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.activation(self.fc1(x))\n",
        "#         out = self.fc2(self.dropout(out))\n",
        "#         return out\n",
        "\n",
        "\n",
        "# class EncoderLayer(torch.nn.Module):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         d_model=768,\n",
        "#         heads=12,\n",
        "#         feed_forward_hidden=768 * 4,\n",
        "#         dropout=0.1\n",
        "#     ):\n",
        "#         super().__init__()\n",
        "#         self.layernorm1 = torch.nn.LayerNorm(d_model)\n",
        "#         self.layernorm2 = torch.nn.LayerNorm(d_model)\n",
        "#         self.self_multihead = MultiHeadedAttention(heads, d_model, dropout=dropout)\n",
        "#         self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden, dropout=dropout)\n",
        "#         self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, embeddings, mask):\n",
        "#         # embeddings: (batch_size, seq_len, d_model)\n",
        "#         # mask:       (batch_size, 1, 1, seq_len)\n",
        "\n",
        "#         attended = self.self_multihead(embeddings, embeddings, embeddings, mask)\n",
        "#         attended = self.dropout(attended)\n",
        "#         out1 = self.layernorm1(attended + embeddings)\n",
        "\n",
        "#         ff = self.feed_forward(out1)\n",
        "#         ff = self.dropout(ff)\n",
        "#         out2 = self.layernorm2(ff + out1)\n",
        "\n",
        "#         return out2\n",
        "\n",
        "\n",
        "# # ==========================\n",
        "# # 3) BERT backbone\n",
        "# # ==========================\n",
        "\n",
        "# class BERT(torch.nn.Module):\n",
        "#     \"\"\"\n",
        "#     BERT model (encoder only).\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, vocab_size, d_model=768, n_layers=10, heads=12, dropout=0.1, seq_len=64):\n",
        "#         super().__init__()\n",
        "#         self.d_model = d_model\n",
        "#         self.n_layers = n_layers\n",
        "#         self.heads = heads\n",
        "\n",
        "#         # Slightly smaller FFN: 3 * d_model (instead of 4 * d_model)\n",
        "#         self.feed_forward_hidden = d_model * 3\n",
        "\n",
        "#         self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model, seq_len=seq_len, dropout=dropout)\n",
        "\n",
        "#         self.encoder_blocks = torch.nn.ModuleList(\n",
        "#             [\n",
        "#                 EncoderLayer(\n",
        "#                     d_model=d_model,\n",
        "#                     heads=heads,\n",
        "#                     feed_forward_hidden=self.feed_forward_hidden,\n",
        "#                     dropout=dropout\n",
        "#                 )\n",
        "#                 for _ in range(n_layers)\n",
        "#             ]\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, segment_info):\n",
        "#         # x: (batch_size, seq_len)\n",
        "#         # segment_info: (batch_size, seq_len)\n",
        "\n",
        "#         # mask: (batch_size, 1, 1, seq_len)\n",
        "#         mask = (x > 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "#         # embedding: (batch_size, seq_len, d_model)\n",
        "#         x = self.embedding(x, segment_info)\n",
        "\n",
        "#         for encoder in self.encoder_blocks:\n",
        "#             x = encoder(x, mask)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # ==========================\n",
        "# # 4) Heads: NSP + MLM + BERTLM\n",
        "# # ==========================\n",
        "\n",
        "# class NextSentencePrediction(torch.nn.Module):\n",
        "#     def __init__(self, hidden):\n",
        "#         super().__init__()\n",
        "#         self.linear = torch.nn.Linear(hidden, 2)\n",
        "#         self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # use [CLS] token embedding\n",
        "#         return self.softmax(self.linear(x[:, 0]))\n",
        "\n",
        "\n",
        "# class MaskedLanguageModel(torch.nn.Module):\n",
        "#     def __init__(self, hidden, vocab_size):\n",
        "#         super().__init__()\n",
        "#         self.linear = torch.nn.Linear(hidden, vocab_size)\n",
        "#         self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x: (batch_size, seq_len, hidden)\n",
        "#         return self.softmax(self.linear(x))\n",
        "\n",
        "\n",
        "# class BERTLM(torch.nn.Module):\n",
        "#     \"\"\"\n",
        "#     Joint model: BERT backbone + NSP head + MLM head\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, bert: BERT, vocab_size):\n",
        "#         super().__init__()\n",
        "#         self.bert = bert\n",
        "#         self.next_sentence = NextSentencePrediction(self.bert.d_model)\n",
        "#         self.mask_lm = MaskedLanguageModel(self.bert.d_model, vocab_size)\n",
        "\n",
        "#     def forward(self, x, segment_label):\n",
        "#         x = self.bert(x, segment_label)\n",
        "#         return self.next_sentence(x), self.mask_lm(x)\n",
        "\n",
        "\n",
        "# # ==========================\n",
        "# # 5) Optimizer + Trainer\n",
        "# # ==========================\n",
        "\n",
        "# class ScheduledOptim:\n",
        "#     \"\"\"\n",
        "#     Simple wrapper for learning rate scheduling (Noam-style).\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "#         self._optimizer = optimizer\n",
        "#         self.n_warmup_steps = n_warmup_steps\n",
        "#         self.n_current_steps = 0\n",
        "#         self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "#     def step_and_update_lr(self):\n",
        "#         self._update_learning_rate()\n",
        "#         self._optimizer.step()\n",
        "\n",
        "#     def zero_grad(self):\n",
        "#         self._optimizer.zero_grad()\n",
        "\n",
        "#     def _get_lr_scale(self):\n",
        "#         return np.min([\n",
        "#             np.power(self.n_current_steps, -0.5),\n",
        "#             np.power(self.n_warmup_steps, -1.5) * self.n_current_steps\n",
        "#         ])\n",
        "\n",
        "#     def _update_learning_rate(self):\n",
        "#         self.n_current_steps += 1\n",
        "#         lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "#         for param_group in self._optimizer.param_groups:\n",
        "#             param_group['lr'] = lr\n",
        "\n",
        "\n",
        "# class BERTTrainer:\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         model,\n",
        "#         train_dataloader,\n",
        "#         test_dataloader=None,\n",
        "#         lr=1e-4,\n",
        "#         weight_decay=0.01,\n",
        "#         betas=(0.9, 0.999),\n",
        "#         warmup_steps=800,          # changed from 10000 to 800\n",
        "#         log_freq=10,\n",
        "#         device='cuda'\n",
        "#     ):\n",
        "#         self.device = device\n",
        "#         self.model = model.to(device)\n",
        "#         self.train_data = train_dataloader\n",
        "#         self.test_data = test_dataloader\n",
        "\n",
        "#         # Optimizer + schedule\n",
        "#         self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "#         self.optim_schedule = ScheduledOptim(\n",
        "#             self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps\n",
        "#         )\n",
        "\n",
        "#         # SEPARATE LOSSES:\n",
        "#         # - NSP: no ignore_index\n",
        "#         # - MLM: ignore_index=0 (PAD / non-predicted tokens)\n",
        "#         self.nsp_criterion = torch.nn.NLLLoss()\n",
        "#         self.mlm_criterion = torch.nn.NLLLoss(ignore_index=0)\n",
        "\n",
        "#         self.log_freq = log_freq\n",
        "#         print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
        "\n",
        "#     def train(self, epoch):\n",
        "#         self.iteration(epoch, self.train_data, train=True)\n",
        "\n",
        "#     def test(self, epoch):\n",
        "#         if self.test_data is not None:\n",
        "#             self.iteration(epoch, self.test_data, train=False)\n",
        "\n",
        "#     def iteration(self, epoch, data_loader, train=True):\n",
        "#         avg_loss = 0.0\n",
        "#         avg_nsp_loss = 0.0\n",
        "#         avg_mlm_loss = 0.0\n",
        "#         total_correct = 0\n",
        "#         total_element = 0\n",
        "\n",
        "#         mode = \"train\" if train else \"test\"\n",
        "\n",
        "#         data_iter = tqdm.tqdm(\n",
        "#             enumerate(data_loader),\n",
        "#             desc=f\"EP_{mode}:{epoch}\",\n",
        "#             total=len(data_loader),\n",
        "#             bar_format=\"{l_bar}{r_bar}\"\n",
        "#         )\n",
        "\n",
        "#         for i, batch in data_iter:\n",
        "#             # Move batch to device\n",
        "#             batch = {key: value.to(self.device) for key, value in batch.items()}\n",
        "\n",
        "#             # Forward\n",
        "#             next_sent_output, mask_lm_output = self.model(\n",
        "#                 batch[\"bert_input\"], batch[\"segment_label\"]\n",
        "#             )\n",
        "\n",
        "#             # Losses\n",
        "#             nsp_loss = self.nsp_criterion(next_sent_output, batch[\"is_next\"])\n",
        "#             mlm_loss = self.mlm_criterion(\n",
        "#                 mask_lm_output.transpose(1, 2), batch[\"bert_label\"]\n",
        "#             )\n",
        "#             loss = nsp_loss + mlm_loss\n",
        "\n",
        "#             if train:\n",
        "#                 self.optim_schedule.zero_grad()\n",
        "#                 loss.backward()\n",
        "#                 self.optim_schedule.step_and_update_lr()\n",
        "\n",
        "#             # NSP accuracy\n",
        "#             correct = next_sent_output.argmax(dim=-1).eq(batch[\"is_next\"]).sum().item()\n",
        "#             total_correct += correct\n",
        "#             total_element += batch[\"is_next\"].nelement()\n",
        "\n",
        "#             avg_loss += loss.item()\n",
        "#             avg_nsp_loss += nsp_loss.item()\n",
        "#             avg_mlm_loss += mlm_loss.item()\n",
        "\n",
        "#             if i % self.log_freq == 0:\n",
        "#                 post_fix = {\n",
        "#                     \"epoch\": epoch,\n",
        "#                     \"iter\": i,\n",
        "#                     \"mode\": mode,\n",
        "#                     \"avg_loss\": avg_loss / (i + 1),\n",
        "#                     \"avg_nsp_loss\": avg_nsp_loss / (i + 1),\n",
        "#                     \"avg_mlm_loss\": avg_mlm_loss / (i + 1),\n",
        "#                     \"avg_acc\": total_correct / total_element * 100,\n",
        "#                     \"loss\": loss.item()\n",
        "#                 }\n",
        "#                 data_iter.write(str(post_fix))\n",
        "\n",
        "#             # W&B logging occasionally\n",
        "#             if i % 300 == 0 and train:\n",
        "#                 metrics = {\n",
        "#                     f\"{mode}/avg_loss\": avg_loss / (i + 1),\n",
        "#                     f\"{mode}/avg_nsp_loss\": avg_nsp_loss / (i + 1),\n",
        "#                     f\"{mode}/avg_mlm_loss\": avg_mlm_loss / (i + 1),\n",
        "#                     f\"{mode}/avg_acc\": total_correct / total_element * 100,\n",
        "#                 }\n",
        "#                 wandb.log(metrics)\n",
        "\n",
        "#         print(\n",
        "#             f\"EP{epoch}, {mode}: \"\n",
        "#             f\"avg_loss={avg_loss / len(data_iter):.4f}, \"\n",
        "#             f\"nsp_loss={avg_nsp_loss / len(data_iter):.4f}, \"\n",
        "#             f\"mlm_loss={avg_mlm_loss / len(data_iter):.4f}, \"\n",
        "#             f\"total_acc={total_correct * 100.0 / total_element:.2f}\"\n",
        "#         )\n",
        "\n",
        "\n",
        "# # ==========================\n",
        "# # 6) Save / load helpers\n",
        "# # ==========================\n",
        "\n",
        "# def save_model(model, optimizer, metrics, epoch, path):\n",
        "#     torch.save(\n",
        "#         {\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'metric': metrics,\n",
        "#             'epoch': epoch\n",
        "#         },\n",
        "#         path\n",
        "#     )\n",
        "\n",
        "# def load_model(model, optimizer=None, path='./checkpoint.pth'):\n",
        "#     checkpoint = torch.load(path, map_location='cpu')\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     if optimizer is not None:\n",
        "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#     epoch = checkpoint['epoch']\n",
        "#     metrics = checkpoint['metric']\n",
        "#     return model, optimizer, epoch, metrics\n",
        "\n",
        "\n",
        "# # ==========================\n",
        "# # 7) Build dataset & dataloader (using your email_pairs + tokenizer)\n",
        "# # ==========================\n",
        "\n",
        "# MAX_LEN = 64  # same as before\n",
        "\n",
        "# train_data = BERTDataset(email_pairs, tokenizer, seq_len=MAX_LEN)\n",
        "# train_loader = DataLoader(train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
        "\n",
        "# sample_batch = next(iter(train_loader))\n",
        "# print(\"Batch shapes:\", {k: v.shape for k, v in sample_batch.items()})\n",
        "\n",
        "\n",
        "# # ==========================\n",
        "# # 8) Init W&B, Model, Trainer, Training Loop\n",
        "# # ==========================\n",
        "\n",
        "# # Login once at the start of notebook (you might have already done this)\n",
        "# wandb.login()  # comment out if you don't want W&B tracking\n",
        "\n",
        "# run = wandb.init(\n",
        "#     name=\"bert-email-pretraining-small\",\n",
        "#     project=\"bert-email-project\",\n",
        "# )\n",
        "\n",
        "# print(\"Initializing smaller BERT model (~100M params)...\")\n",
        "# vocab_size = len(tokenizer.vocab)\n",
        "\n",
        "# bert_model = BERT(\n",
        "#     vocab_size=vocab_size,\n",
        "#     d_model=768,\n",
        "#     n_layers=10,   # smaller than 12\n",
        "#     heads=12,\n",
        "#     dropout=0.1,\n",
        "#     seq_len=MAX_LEN\n",
        "# )\n",
        "\n",
        "# bert_lm = BERTLM(bert_model, vocab_size=vocab_size)\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# bert_trainer = BERTTrainer(\n",
        "#     bert_lm,\n",
        "#     train_loader,\n",
        "#     device=device,\n",
        "#     warmup_steps=800\n",
        "# )\n",
        "\n",
        "# # ==========================\n",
        "# # 9) Train\n",
        "# # ==========================\n",
        "\n",
        "# epochs = 5\n",
        "# ckpt_dir = '/content/drive/MyDrive/email_bert/checkpoints'\n",
        "# os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     bert_trainer.train(epoch)\n",
        "\n",
        "#     epoch_path = os.path.join(ckpt_dir, f'bert_email_epoch_{epoch}.pth')\n",
        "#     save_model(bert_lm, bert_trainer.optim, {\"epoch\": epoch}, epoch, epoch_path)\n",
        "#     print(f\"Saved model after epoch {epoch} to {epoch_path}\")\n",
        "\n",
        "# # Final model\n",
        "# final_path = '/content/drive/MyDrive/email_bert/bert_email_final_small.pth'\n",
        "# save_model(bert_lm, bert_trainer.optim, {\"final_epoch\": epochs}, epochs, final_path)\n",
        "# print(f\"Pre-training complete. Final model saved to {final_path}\")\n",
        "\n",
        "# wandb.finish()\n",
        "\n",
        "# print(\"\\nModel and data files are saved to Google Drive in the following locations:\")\n",
        "# print(\"- Dataset: /content/drive/MyDrive/email_bert/datasets/\")\n",
        "# print(\"- Tokenizer: /content/drive/MyDrive/email_bert/tokenizer/\")\n",
        "# print(\"- Checkpoints: /content/drive/MyDrive/email_bert/checkpoints/\")\n",
        "# print(\"- Final model: /content/drive/MyDrive/email_bert/bert_email_final_small.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a-Im_CZYMjby",
        "outputId": "bc0c8b72-f898-4198-d791-d9e839513897"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Enron emails...\n",
            "Created 43372 email pairs\n",
            "Sample pair: ['If you are not the named addressee, you are not authorized to read, print, retain, copy or disseminate this message or any part of it', 'If you have received this message in error, please notify the sender immediately by e-mail and delete all copies of the message']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 86744/86744 [00:00<00:00, 1729652.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 9 text files for tokenizer training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample tokenization: ['[CLS]', 'this', 'is', 'an', 'email', 'about', 'the', 'project', 'deadline', 'tomorrow', '[UNK]', '[SEP]']\n",
            "Batch shapes: {'bert_input': torch.Size([32, 64]), 'bert_label': torch.Size([32, 64]), 'segment_label': torch.Size([32, 64]), 'is_next': torch.Size([32])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maarunmoz\u001b[0m (\u001b[33maarunmoz-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251202_045403-st96nc8q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aarunmoz-carnegie-mellon-university/bert-email-project/runs/st96nc8q' target=\"_blank\">bert-email-pretraining-small</a></strong> to <a href='https://wandb.ai/aarunmoz-carnegie-mellon-university/bert-email-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/aarunmoz-carnegie-mellon-university/bert-email-project' target=\"_blank\">https://wandb.ai/aarunmoz-carnegie-mellon-university/bert-email-project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/aarunmoz-carnegie-mellon-university/bert-email-project/runs/st96nc8q' target=\"_blank\">https://wandb.ai/aarunmoz-carnegie-mellon-university/bert-email-project/runs/st96nc8q</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing smaller BERT model (~100M params)...\n",
            "Total Parameters: 105188402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 1/1356 [00:05<1:54:41,  5.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 0, 'mode': 'train', 'avg_loss': 11.087310791015625, 'avg_nsp_loss': 0.708968997001648, 'avg_mlm_loss': 10.378341674804688, 'avg_acc': 50.0, 'loss': 11.087310791015625}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   1%|| 11/1356 [00:46<1:28:27,  3.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 10, 'mode': 'train', 'avg_loss': 11.080776474692605, 'avg_nsp_loss': 0.7543819167397239, 'avg_mlm_loss': 10.326394514604049, 'avg_acc': 48.86363636363637, 'loss': 10.925771713256836}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   2%|| 21/1356 [01:19<1:16:36,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 20, 'mode': 'train', 'avg_loss': 10.694304784138998, 'avg_nsp_loss': 0.7821678746314276, 'avg_mlm_loss': 9.912136895315987, 'avg_acc': 48.06547619047619, 'loss': 9.484539985656738}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   2%|| 31/1356 [01:54<1:11:38,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 30, 'mode': 'train', 'avg_loss': 10.443192574285693, 'avg_nsp_loss': 0.7744970783110587, 'avg_mlm_loss': 9.66869548059279, 'avg_acc': 49.193548387096776, 'loss': 9.97372817993164}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   3%|| 41/1356 [02:29<1:13:44,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 40, 'mode': 'train', 'avg_loss': 10.235857847260267, 'avg_nsp_loss': 0.760804558672556, 'avg_mlm_loss': 9.475053298764113, 'avg_acc': 50.0, 'loss': 10.009326934814453}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   4%|| 51/1356 [03:00<1:10:21,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 50, 'mode': 'train', 'avg_loss': 10.108970922582289, 'avg_nsp_loss': 0.7520189098283356, 'avg_mlm_loss': 9.356952012753954, 'avg_acc': 50.0, 'loss': 9.594722747802734}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   4%|| 61/1356 [03:32<1:07:11,  3.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 60, 'mode': 'train', 'avg_loss': 9.966916334433634, 'avg_nsp_loss': 0.7466525669957771, 'avg_mlm_loss': 9.220263778186235, 'avg_acc': 50.409836065573764, 'loss': 8.729504585266113}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   5%|| 71/1356 [04:07<1:28:28,  4.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 70, 'mode': 'train', 'avg_loss': 9.873166420090367, 'avg_nsp_loss': 0.7637243245688963, 'avg_mlm_loss': 9.109442106434997, 'avg_acc': 50.0, 'loss': 8.97524642944336}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   6%|| 81/1356 [04:51<1:33:38,  4.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 80, 'mode': 'train', 'avg_loss': 9.739439493344154, 'avg_nsp_loss': 0.7672536461441605, 'avg_mlm_loss': 8.972185870747508, 'avg_acc': 49.84567901234568, 'loss': 8.170945167541504}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   7%|| 91/1356 [05:34<1:39:50,  4.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 90, 'mode': 'train', 'avg_loss': 9.607194460355318, 'avg_nsp_loss': 0.7669792961288284, 'avg_mlm_loss': 8.840215190426333, 'avg_acc': 49.65659340659341, 'loss': 7.355524063110352}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   7%|| 101/1356 [06:23<1:41:35,  4.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 100, 'mode': 'train', 'avg_loss': 9.47360620404234, 'avg_nsp_loss': 0.761820630861981, 'avg_mlm_loss': 8.711785595015725, 'avg_acc': 49.10272277227723, 'loss': 8.306102752685547}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   8%|| 111/1356 [07:00<1:07:46,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'iter': 110, 'mode': 'train', 'avg_loss': 9.36255492390813, 'avg_nsp_loss': 0.7568679758020349, 'avg_mlm_loss': 8.605686965289417, 'avg_acc': 49.07094594594595, 'loss': 8.720757484436035}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   9%|| 119/1356 [07:25<1:17:08,  3.74s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2635362277.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m     \u001b[0mbert_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0mepoch_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'bert_email_epoch_{epoch}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2635362277.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2635362277.py\u001b[0m in \u001b[0;36miteration\u001b[0;34m(self, epoch, data_loader, train)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         )\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m             \u001b[0;31m# Move batch to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2635362277.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;31m# Step 2: replace random words with mask / random words (for MLM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0mt1_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[0mt2_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;31m# Step 3: Add special tokens (CLS, SEP)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2635362277.py\u001b[0m in \u001b[0;36mrandom_word\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'[MASK]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0;31m# 10%: random token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36mvocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36mget_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_added_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n",
        "enron_dir = '/content/maildir/'\n",
        "\n",
        "import os, re, email, tqdm\n",
        "\n",
        "MAX_LEN = 64\n",
        "email_pairs = []\n",
        "\n",
        "def fast_body_extract(path):\n",
        "    try:\n",
        "        with open(path, \"r\", errors=\"ignore\") as f:\n",
        "            msg = email.message_from_string(f.read())\n",
        "        if msg.is_multipart():\n",
        "            for part in msg.walk():\n",
        "                if part.get_content_type() == \"text/plain\":\n",
        "                    try: return part.get_payload(decode=True).decode(\"latin1\")\n",
        "                    except: return \"\"\n",
        "        else:\n",
        "            try: return msg.get_payload(decode=True).decode(\"latin1\")\n",
        "            except: return \"\"\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "print(\"Processing emails...\")\n",
        "\n",
        "users = os.listdir(enron_dir)[:30]\n",
        "for u in users:\n",
        "    udir = os.path.join(enron_dir, u)\n",
        "    if not os.path.isdir(udir): continue\n",
        "    for folder in os.listdir(udir)[:5]:\n",
        "        fdir = os.path.join(udir, folder)\n",
        "        if not os.path.isdir(fdir): continue\n",
        "        for file in os.listdir(fdir)[:50]:\n",
        "            fpath = os.path.join(fdir, file)\n",
        "            body = fast_body_extract(fpath)\n",
        "            if not body: continue\n",
        "\n",
        "            sents = re.split(r'[.!?]+', body)\n",
        "            sents = [s.strip() for s in sents if len(s.strip()) > 10]\n",
        "\n",
        "            for i in range(len(sents)-1):\n",
        "                email_pairs.append([\n",
        "                    \" \".join(sents[i].split()[:MAX_LEN]),\n",
        "                    \" \".join(sents[i+1].split()[:MAX_LEN])\n",
        "                ])\n",
        "\n",
        "print(\"Pairs:\", len(email_pairs))\n",
        "from pathlib import Path\n",
        "\n",
        "path = \"/content/drive/MyDrive/email_bert/data/\"\n",
        "os.makedirs(path, exist_ok=True)\n",
        "\n",
        "chunks = []\n",
        "chunk_id = 0\n",
        "\n",
        "for t1, t2 in tqdm.tqdm(email_pairs):\n",
        "    chunks.append(t1)\n",
        "    chunks.append(t2)\n",
        "    if len(chunks) >= 10000:\n",
        "        with open(f\"{path}/text_{chunk_id}.txt\", \"w\") as f:\n",
        "            f.write(\"\\n\".join(chunks))\n",
        "        chunks, chunk_id = [], chunk_id+1\n",
        "\n",
        "if chunks:\n",
        "    with open(f\"{path}/text_{chunk_id}.txt\", \"w\") as f:\n",
        "        f.write(\"\\n\".join(chunks))\n",
        "\n",
        "paths = [str(x) for x in Path(path).glob(\"*.txt\")]\n",
        "print(\"Text files:\", len(paths))\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "tok = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=False,\n",
        "    strip_accents=False,\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "tok.train(\n",
        "    files=paths,\n",
        "    vocab_size=30000,\n",
        "    min_frequency=2,\n",
        "    limit_alphabet=1000,\n",
        "    special_tokens=[\"[PAD]\",\"[CLS]\",\"[SEP]\",\"[MASK]\",\"[UNK]\"]\n",
        ")\n",
        "\n",
        "tok.save_model(\"/content/drive/MyDrive/email_bert/tokenizer\",\"bert-email\")\n",
        "\n",
        "from transformers import BertTokenizerFast\n",
        "tokenizer = BertTokenizerFast(\n",
        "    vocab_file=\"/content/drive/MyDrive/email_bert/tokenizer/bert-email-vocab.txt\",\n",
        "    lowercase=True,\n",
        "    strip_accents=False\n",
        ")\n",
        "\n",
        "print(\"Tokenizer size:\", len(tokenizer))\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class EmailBERTDataset(Dataset):\n",
        "    def __init__(self, pairs, tokenizer, seq_len=64):\n",
        "        self.pairs = pairs\n",
        "        self.tok = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = len(tokenizer)\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        t1, t2 = self.pairs[i]\n",
        "\n",
        "        # 50% random NSP replacement\n",
        "        if torch.rand(1).item() < 0.5:\n",
        "            is_next = 1\n",
        "        else:\n",
        "            t2 = self.pairs[ torch.randint(len(self.pairs),(1,)).item() ][1]\n",
        "            is_next = 0\n",
        "\n",
        "        out = self.tok(\n",
        "            t1, t2,\n",
        "            max_length=self.seq_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        ids = out[\"input_ids\"].squeeze()\n",
        "        seg = out[\"token_type_ids\"].squeeze()\n",
        "\n",
        "        # Create MLM mask\n",
        "        mask_prob = 0.15\n",
        "        rand = torch.rand(ids.shape)\n",
        "\n",
        "        mask = (rand < mask_prob) & (ids != self.tok.pad_token_id) & (ids != self.tok.cls_token_id) & (ids != self.tok.sep_token_id)\n",
        "        labels = ids.clone()\n",
        "        labels[~mask] = -100\n",
        "\n",
        "        # Replace masked tokens\n",
        "        ids_masked = ids.clone()\n",
        "        ids_masked[mask] = self.tok.mask_token_id\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": ids_masked,\n",
        "            \"token_type_ids\": seg,\n",
        "            \"labels\": labels,\n",
        "            \"is_next\": torch.tensor(is_next)\n",
        "        }\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = EmailBERTDataset(email_pairs, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "print(\"Batch OK:\", batch[\"input_ids\"].shape)\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "class BERTSmall(nn.Module):\n",
        "    def __init__(self, vocab, d_model=512, layers=8, heads=8, seq_len=64):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, d_model)\n",
        "        self.pos = nn.Embedding(seq_len, d_model)\n",
        "        self.seg = nn.Embedding(2, d_model)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=heads,\n",
        "            dim_feedforward=d_model*3,\n",
        "            batch_first=True,\n",
        "            activation=\"gelu\"\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=layers)\n",
        "\n",
        "        self.mlm = nn.Linear(d_model, vocab)\n",
        "        self.nsp = nn.Linear(d_model, 2)\n",
        "\n",
        "    def forward(self, ids, seg):\n",
        "        b, L = ids.shape\n",
        "        pos = torch.arange(L, device=ids.device).unsqueeze(0)\n",
        "\n",
        "        x = self.emb(ids) + self.pos(pos) + self.seg(seg)\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        mlm_logits = self.mlm(x)\n",
        "        nsp_logits = self.nsp(x[:,0])\n",
        "        return mlm_logits, nsp_logits\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, device=\"cuda\"):\n",
        "        self.model = model.to(device)\n",
        "        self.loader = loader\n",
        "        self.device = device\n",
        "        self.opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "        self.scaler = GradScaler()\n",
        "        self.mlm_loss = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "        self.nsp_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    import os\n",
        "\n",
        "# -------------------------\n",
        "# Count model parameters\n",
        "# -------------------------\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "#  UPDATED TRAINING FUNCTION WITH METRICS & CHECKPOINTS\n",
        "# ==========================================================\n",
        "def train_with_checkpoints(trainer, epochs=100, ckpt_dir=\"/content/drive/MyDrive/email_bert/checkpoints_small/\"):\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"🚀 Training Started\")\n",
        "    print(\"Total Trainable Parameters:\", count_parameters(trainer.model))\n",
        "    print(\"==============================\\n\")\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        mlm_loss_sum = 0\n",
        "        nsp_loss_sum = 0\n",
        "        total_loss_sum = 0\n",
        "        steps = 0\n",
        "\n",
        "        for batch in tqdm.tqdm(trainer.loader, desc=f\"Epoch {ep}\"):\n",
        "            ids = batch[\"input_ids\"].to(trainer.device)\n",
        "            seg = batch[\"token_type_ids\"].to(trainer.device)\n",
        "            mlm_labels = batch[\"labels\"].to(trainer.device)\n",
        "            nsp_labels = batch[\"is_next\"].to(trainer.device)\n",
        "\n",
        "            with autocast():\n",
        "                mlm_logits, nsp_logits = trainer.model(ids, seg)\n",
        "                loss_mlm = trainer.mlm_loss(\n",
        "                    mlm_logits.view(-1, mlm_logits.size(-1)),\n",
        "                    mlm_labels.view(-1)\n",
        "                )\n",
        "                loss_nsp = trainer.nsp_loss(nsp_logits, nsp_labels)\n",
        "                loss = loss_mlm + loss_nsp\n",
        "\n",
        "            trainer.opt.zero_grad()\n",
        "            trainer.scaler.scale(loss).backward()\n",
        "            trainer.scaler.step(trainer.opt)\n",
        "            trainer.scaler.update()\n",
        "\n",
        "            # Metrics\n",
        "            correct += (nsp_logits.argmax(-1) == nsp_labels).sum().item()\n",
        "            total += ids.size(0)\n",
        "\n",
        "            mlm_loss_sum += loss_mlm.item()\n",
        "            nsp_loss_sum += loss_nsp.item()\n",
        "            total_loss_sum += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        # ---- END OF EPOCH METRICS ----\n",
        "        avg_mlm = mlm_loss_sum / steps\n",
        "        avg_nsp = nsp_loss_sum / steps\n",
        "        avg_total = total_loss_sum / steps\n",
        "        acc = correct / total * 100\n",
        "\n",
        "        print(f\"\\n📊 Epoch {ep} Summary\")\n",
        "        print(f\"  MLM Loss: {avg_mlm:.4f}\")\n",
        "        print(f\"  NSP Loss: {avg_nsp:.4f}\")\n",
        "        print(f\"  Total Loss: {avg_total:.4f}\")\n",
        "        print(f\"  NSP Accuracy: {acc:.2f}%\")\n",
        "        print(\"------------------------------\")\n",
        "\n",
        "        # Save every 10 epochs\n",
        "        if (ep + 1) % 10 == 0:\n",
        "            save_path = os.path.join(ckpt_dir, f\"bert_email_epoch_{ep}.pth\")\n",
        "            torch.save(trainer.model.state_dict(), save_path)\n",
        "            print(f\"💾 Saved checkpoint: {save_path}\")\n",
        "\n",
        "    # Save final model\n",
        "    final_path = \"/content/drive/MyDrive/email_bert/bert_email_83M_final.pth\"\n",
        "    torch.save(trainer.model.state_dict(), final_path)\n",
        "    print(f\"\\n🎉 Final model saved at: {final_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv6NFC8DcFGM",
        "outputId": "3826f438-22bd-4e4c-f60e-bc48660c65da"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  enron_mail_20150507.tar.gz  maildir  sample_data  wandb\n",
            "Processing emails...\n",
            "Pairs: 56290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 56290/56290 [00:00<00:00, 1284415.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text files: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer size: 30000\n",
            "Batch OK: torch.Size([32, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_with_checkpoints(trainer, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q8wwZHXBg0Xr",
        "outputId": "3b931f3c-a452-4e3a-8edf-bad397d2cff6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "🚀 Training Started\n",
            "Total Trainable Parameters: 51805490\n",
            "==============================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:   0%|          | 0/1760 [00:00<?, ?it/s]/tmp/ipython-input-1701784820.py:238: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 0: 100%|██████████| 1760/1760 [00:52<00:00, 33.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 0 Summary\n",
            "  MLM Loss: 6.9423\n",
            "  NSP Loss: 0.6940\n",
            "  Total Loss: 7.6363\n",
            "  NSP Accuracy: 50.19%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 1760/1760 [00:52<00:00, 33.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 1 Summary\n",
            "  MLM Loss: 6.9402\n",
            "  NSP Loss: 0.6941\n",
            "  Total Loss: 7.6343\n",
            "  NSP Accuracy: 49.61%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 1760/1760 [00:51<00:00, 34.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 2 Summary\n",
            "  MLM Loss: 6.9431\n",
            "  NSP Loss: 0.6938\n",
            "  Total Loss: 7.6368\n",
            "  NSP Accuracy: 50.21%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 1760/1760 [00:51<00:00, 34.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 3 Summary\n",
            "  MLM Loss: 6.9370\n",
            "  NSP Loss: 0.6938\n",
            "  Total Loss: 7.6308\n",
            "  NSP Accuracy: 50.03%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 1760/1760 [00:51<00:00, 34.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 4 Summary\n",
            "  MLM Loss: 6.9357\n",
            "  NSP Loss: 0.6938\n",
            "  Total Loss: 7.6296\n",
            "  NSP Accuracy: 49.85%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 1760/1760 [00:51<00:00, 34.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 5 Summary\n",
            "  MLM Loss: 6.9335\n",
            "  NSP Loss: 0.6937\n",
            "  Total Loss: 7.6272\n",
            "  NSP Accuracy: 49.82%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 1760/1760 [00:51<00:00, 34.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 6 Summary\n",
            "  MLM Loss: 6.9334\n",
            "  NSP Loss: 0.6937\n",
            "  Total Loss: 7.6270\n",
            "  NSP Accuracy: 50.17%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 1760/1760 [00:51<00:00, 34.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 7 Summary\n",
            "  MLM Loss: 6.9394\n",
            "  NSP Loss: 0.6938\n",
            "  Total Loss: 7.6332\n",
            "  NSP Accuracy: 49.89%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 1760/1760 [00:51<00:00, 33.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 8 Summary\n",
            "  MLM Loss: 6.9256\n",
            "  NSP Loss: 0.6936\n",
            "  Total Loss: 7.6192\n",
            "  NSP Accuracy: 50.08%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 1760/1760 [00:51<00:00, 34.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 9 Summary\n",
            "  MLM Loss: 6.9388\n",
            "  NSP Loss: 0.6936\n",
            "  Total Loss: 7.6325\n",
            "  NSP Accuracy: 49.79%\n",
            "------------------------------\n",
            "💾 Saved checkpoint: /content/drive/MyDrive/email_bert/checkpoints_small/bert_email_epoch_9.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 1760/1760 [00:52<00:00, 33.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 10 Summary\n",
            "  MLM Loss: 6.9343\n",
            "  NSP Loss: 0.6935\n",
            "  Total Loss: 7.6278\n",
            "  NSP Accuracy: 50.36%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 1760/1760 [00:50<00:00, 34.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 11 Summary\n",
            "  MLM Loss: 6.9250\n",
            "  NSP Loss: 0.6936\n",
            "  Total Loss: 7.6186\n",
            "  NSP Accuracy: 49.90%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 1760/1760 [00:51<00:00, 34.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 12 Summary\n",
            "  MLM Loss: 6.9276\n",
            "  NSP Loss: 0.6936\n",
            "  Total Loss: 7.6212\n",
            "  NSP Accuracy: 50.04%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 1760/1760 [00:52<00:00, 33.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 13 Summary\n",
            "  MLM Loss: 6.9329\n",
            "  NSP Loss: 0.6935\n",
            "  Total Loss: 7.6264\n",
            "  NSP Accuracy: 50.32%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 1760/1760 [00:51<00:00, 34.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 14 Summary\n",
            "  MLM Loss: 6.9378\n",
            "  NSP Loss: 0.6936\n",
            "  Total Loss: 7.6314\n",
            "  NSP Accuracy: 49.86%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 1760/1760 [00:51<00:00, 34.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 15 Summary\n",
            "  MLM Loss: 6.9266\n",
            "  NSP Loss: 0.6934\n",
            "  Total Loss: 7.6200\n",
            "  NSP Accuracy: 50.09%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 1760/1760 [00:51<00:00, 34.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 16 Summary\n",
            "  MLM Loss: 6.9288\n",
            "  NSP Loss: 0.6936\n",
            "  Total Loss: 7.6224\n",
            "  NSP Accuracy: 49.91%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 1760/1760 [00:52<00:00, 33.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 17 Summary\n",
            "  MLM Loss: 6.9232\n",
            "  NSP Loss: 0.6934\n",
            "  Total Loss: 7.6166\n",
            "  NSP Accuracy: 50.35%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 1760/1760 [00:51<00:00, 33.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 18 Summary\n",
            "  MLM Loss: 6.9176\n",
            "  NSP Loss: 0.6934\n",
            "  Total Loss: 7.6110\n",
            "  NSP Accuracy: 50.10%\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 1760/1760 [00:52<00:00, 33.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 19 Summary\n",
            "  MLM Loss: 6.9318\n",
            "  NSP Loss: 0.6934\n",
            "  Total Loss: 7.6252\n",
            "  NSP Accuracy: 50.21%\n",
            "------------------------------\n",
            "💾 Saved checkpoint: /content/drive/MyDrive/email_bert/checkpoints_small/bert_email_epoch_19.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20:  32%|███▏      | 570/1760 [00:17<00:36, 32.46it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2696705394.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_with_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1701784820.py\u001b[0m in \u001b[0;36mtrain_with_checkpoints\u001b[0;34m(trainer, epochs, ckpt_dir)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /content/drive\n",
        "!ls -R /content/gdrive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1tOR_qek28f",
        "outputId": "49e8c21f-ee16-46ae-b236-75198b4148da"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive:\n",
            "MyDrive\n",
            "\n",
            "/content/drive/MyDrive:\n",
            "email_bert\n",
            "\n",
            "/content/drive/MyDrive/email_bert:\n",
            "bert_email_83M.pth  checkpoints  checkpoints_small  data  datasets  tokenizer\n",
            "\n",
            "/content/drive/MyDrive/email_bert/checkpoints:\n",
            "\n",
            "/content/drive/MyDrive/email_bert/checkpoints_small:\n",
            "bert_email_epoch_19.pth  bert_email_epoch_9.pth\n",
            "\n",
            "/content/drive/MyDrive/email_bert/data:\n",
            "text_0.txt   text_11.txt  text_2.txt  text_4.txt  text_6.txt  text_8.txt\n",
            "text_10.txt  text_1.txt   text_3.txt  text_5.txt  text_7.txt  text_9.txt\n",
            "\n",
            "/content/drive/MyDrive/email_bert/datasets:\n",
            "email_pairs.pkl  enron_mail_20150507.tar.gz\n",
            "\n",
            "/content/drive/MyDrive/email_bert/tokenizer:\n",
            "bert-email-vocab.txt\n",
            "ls: cannot access '/content/gdrive': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/drive/MyDrive/email_bert/checkpoints_small/bert_email_epoch_9.pth')\n",
        "# or\n",
        "#files.download('/content/drive/MyDrive/email_bert/bert_email_83M.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pTVskMpTl0as",
        "outputId": "e728ee8f-0881-4013-9ced-9b42434fadb0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b2b368f5-f62e-473c-91b8-7a534c7b52a0\", \"bert_email_epoch_9.pth\", 207260832)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /content/drive\n",
        "!ls -R /content/gdrive\n"
      ],
      "metadata": {
        "id": "GqpMIWJbkQkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/drive/MyDrive/email_bert/datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MgLGCQiTPVV",
        "outputId": "69b8fab0-069c-403c-e85e-483c2531c740"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 181M\n",
            "-rw------- 1 root root 9.5M Dec  2 04:22 email_pairs.pkl\n",
            "drwx------ 2 root root 4.0K Oct 21 00:43 enron_mail_20150507\n",
            "-rw------- 1 root root 156M Dec  2 04:22 enron_mail_20150507.tar.gz\n",
            "-rw------- 1 root root  16M Dec  2 02:16 enron_mail_20150507.tar.gz.1\n",
            "drwx------ 3 root root 4.0K Oct 21 00:54 maildir\n",
            "drwx------ 2 root root 4.0K Oct 23 05:04 spamassassin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/drive/MyDrive/email_bert/tokenizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZdDc6c1OQ2X",
        "outputId": "87fdd8c1-ca0b-4781-8589-3349a8488fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 212\n",
            "-rw------- 1 root root 216114 Dec  2 04:00 bert-email-vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uJWvOHMNfRR",
        "outputId": "fc09fcf8-928a-4c77-ed28-492c529dc179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  enron_mail_20150507.tar.gz  maildir  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls -la /content/drive/MyDrive/email_bert/checkpoints/"
      ],
      "metadata": {
        "id": "eZYkqSmaESkC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}